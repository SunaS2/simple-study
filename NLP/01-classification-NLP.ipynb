{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SSAFY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SSAFY\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "# nltk의 불용어와 표제어 추출 준비\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 모델과 토크나이저 로드\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 로드\n",
    "data = pd.read_csv('cefr_leveled_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 전처리 함수 정의\n",
    "def preprocess_text(text):\n",
    "    # 1. 토큰화 및 소문자 변환\n",
    "    tokens = text.lower().split()\n",
    "    \n",
    "    # 2. 불용어 제거\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 3. 표제어 추출\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # 전처리된 텍스트 반환\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터에 전처리 적용\n",
    "data['processed_text'] = data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 임베딩 함수 정의\n",
    "def get_bert_embeddings(text):\n",
    "    # 입력 텍스트를 BERT의 입력 형식으로 변환\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # 임베딩 결과는 모델의 마지막 은닉층\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # 평균 풀링으로 문장 벡터 생성\n",
    "    return embeddings.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label  \\\n",
      "0  Hi!\\r\\nI've been meaning to write for ages and...    B2   \n",
      "1  ﻿It was not so much how hard people found the ...    B2   \n",
      "2  Keith recently came back from a trip to Chicag...    B2   \n",
      "3  The Griffith Observatory is a planetarium, and...    B2   \n",
      "4  -LRB- The Hollywood Reporter -RRB- It's offici...    B2   \n",
      "\n",
      "                                      processed_text  \\\n",
      "0  hi! i've meaning write age finally today i'm a...   \n",
      "1  ﻿it much hard people found challenge far would...   \n",
      "2  keith recently came back trip chicago, illinoi...   \n",
      "3  griffith observatory planetarium, exhibit hall...   \n",
      "4  -lrb- hollywood reporter -rrb- official: amc's...   \n",
      "\n",
      "                                     bert_embeddings  \n",
      "0  [-0.22955546, -0.0098610325, 1.032207, -0.1835...  \n",
      "1  [-0.2589494, 0.044663645, 0.6809145, -0.111927...  \n",
      "2  [-0.44110575, 0.12668371, 0.5283675, -0.103395...  \n",
      "3  [-0.28058243, 0.2910775, 0.62295926, -0.081059...  \n",
      "4  [-0.23377775, -0.14903452, 0.5543672, -0.06348...  \n"
     ]
    }
   ],
   "source": [
    "# BERT 임베딩 생성 및 적용\n",
    "data['bert_embeddings'] = data['processed_text'].apply(get_bert_embeddings)\n",
    "\n",
    "# 결과 확인\n",
    "print(data[['text', 'label', 'processed_text', 'bert_embeddings']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리된 데이터 로드\n",
    "# data = pd.read_csv('./cefr_leveled_texts.csv')  # 실제 파일명으로 변경하세요\n",
    "X = np.stack(data['bert_embeddings'].values)   # BERT 임베딩을 사용한 입력 데이터\n",
    "y = data['label'].values                       # CEFR 레벨 라벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할 (80% 학습용, 20% 테스트용)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=300, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Fold Cross-Validation Accuracy: [0.64016736 0.62761506 0.60669456 0.58995816 0.56066946]\n",
      "Mean Cross-Validation Accuracy: 0.605020920502092\n"
     ]
    }
   ],
   "source": [
    "# 교차 검증을 통한 성능 평가\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(training_model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "\n",
    "print(\"K-Fold Cross-Validation Accuracy:\", cv_scores)\n",
    "print(\"Mean Cross-Validation Accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6655518394648829\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          A1       0.83      0.85      0.84        67\n",
      "          A2       0.73      0.69      0.71        52\n",
      "          B1       0.61      0.47      0.53        36\n",
      "          B2       0.53      0.74      0.62        54\n",
      "          C1       0.57      0.41      0.48        51\n",
      "          C2       0.70      0.72      0.71        39\n",
      "\n",
      "    accuracy                           0.67       299\n",
      "   macro avg       0.66      0.65      0.65       299\n",
      "weighted avg       0.67      0.67      0.66       299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 최종 학습 및 예측\n",
    "training_model.fit(X_train, y_train)\n",
    "y_pred = training_model.predict(X_test)\n",
    "\n",
    "# 모델 평가\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 대사 난이도 예측 함수\n",
    "def predict_difficulty(dialogues):\n",
    "    dialogues_processed = [preprocess_text(dialogue) for dialogue in dialogues]  # 전처리 적용\n",
    "    embeddings = [get_bert_embeddings(dialogue) for dialogue in dialogues_processed]  # BERT 임베딩 적용\n",
    "    predictions = training_model.predict(embeddings)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted CEFR Levels: ['C2']\n"
     ]
    }
   ],
   "source": [
    "# 예시: 새로운 영화 대사 난이도 예측\n",
    "new_dialogues = [\"In view of this, the Ethiopian Government and other developmental partners have introduced an extensive mechanical and biological watershed conservation schemes in various parts of the country over the last decades particularly after the famine of the 1970s\"]\n",
    "difficulty_predictions = predict_difficulty(new_dialogues)\n",
    "print(\"Predicted CEFR Levels:\", difficulty_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
